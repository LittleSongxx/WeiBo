{
  "permissions": {
    "allow": [
      "Bash(python:*)",
      "Bash(source:*)",
      "Bash(conda activate:*)",
      "Bash(conda info:*)",
      "WebFetch(domain:github.com)",
      "Bash(curl:*)",
      "Bash(netstat:*)",
      "Bash(ss:*)",
      "Bash(python3:*)",
      "Bash(tail:*)",
      "Bash(tmux list-sessions:*)",
      "Bash(screen:*)",
      "Bash(TAG_TASK_ID=\"1c852d525228279cdc65fa7b92a02151\" WEIBO_ID=\"5210302282990587\" __NEW_LINE_08e48652ccdbfc1d__ echo \"=== 评论任务状态 ===\" curl -s \"http://127.0.0.1:8181/api/tag/comment/task_status?tag_task_id=$TAG_TASK_ID&weibo_id=$WEIBO_ID\")",
      "Bash(/dev/null __NEW_LINE_08e48652ccdbfc1d__ echo -e '\\\\n=== 评论任务列表 ===' curl -s http://127.0.0.1:8181/api/tag/comment/task_list?tag_task_id=$TAG_TASK_ID)",
      "Bash(TAG_TASK_ID=\"1c852d525228279cdc65fa7b92a02151\")",
      "Bash(WEIBO_ID=\"5210302282990587\")",
      "Bash(__NEW_LINE_b904387c9963c325__ curl -s \"http://127.0.0.1:8181/api/tag/comment/task_status?tag_task_id=$TAG_TASK_ID&weibo_id=$WEIBO_ID\")",
      "Bash(conda run -n graduation python -c \"\nfrom pymongo import MongoClient\n\n# 连接MongoDB\nclient = MongoClient\\(''mongodb://localhost:27017/''\\)\ndb = client[''public_opinion_analysis_system'']\n\n# 查看tag_hot collection的数据\nprint\\(''=== tag_hot collection ===''\\)\nhot_data = list\\(db[''tag_hot''].find\\(\\)\\)\nprint\\(f''总记录数: {len\\(hot_data\\)}''\\)\nfor doc in hot_data[:5]:  # 只显示前5条\n    print\\(doc\\)\n    print\\(''---''\\)\n\n# 查看有哪些collection\nprint\\(''\\\\n=== 所有collections ===''\\)\nprint\\(db.list_collection_names\\(\\)\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport gopup as gp\n\n# 测试gopup是否能获取数据\nprint\\(''测试gopup获取微博指数...''\\)\ntry:\n    result = gp.weibo_index\\(word=''国足'', time_type=''1day''\\)\n    print\\(''结果类型:'', type\\(result\\)\\)\n    print\\(''结果内容:''\\)\n    print\\(result\\)\nexcept Exception as e:\n    print\\(f''错误: {e}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nfrom pymongo import MongoClient\nfrom datetime import datetime, timedelta\n\nclient = MongoClient\\(''mongodb://localhost:27017/''\\)\ndb = client[''public_opinion_analysis_system'']\n\n# 查看blog collection的数据\nprint\\(''=== blog collection ===''\\)\nblog_count = db[''blog''].count_documents\\({}\\)\nprint\\(f''总记录数: {blog_count}''\\)\n\n# 查看数据结构\nprint\\(''\\\\n=== 数据结构示例 ===''\\)\nsample = db[''blog''].find_one\\(\\)\nif sample:\n    print\\(''字段:'', list\\(sample.keys\\(\\)\\)\\)\n    print\\(''示例数据:''\\)\n    for key, value in sample.items\\(\\):\n        if key != ''_id'':\n            print\\(f''  {key}: {str\\(value\\)[:100]}...'' if len\\(str\\(value\\)\\) > 100 else f''  {key}: {value}''\\)\n\n# 查看有哪些tag\nprint\\(''\\\\n=== 不同的tag ===''\\)\ntags = db[''blog''].distinct\\(''tag''\\)\nprint\\(f''tags: {tags}''\\)\n\n# 查看国足相关的数据\nprint\\(''\\\\n=== 国足相关数据 ===''\\)\nguozu_count = db[''blog''].count_documents\\({''tag'': ''国足''}\\)\nprint\\(f''国足相关微博数: {guozu_count}''\\)\n\n# 查看时间分布\nprint\\(''\\\\n=== 时间字段检查 ===''\\)\nsample_with_time = db[''blog''].find_one\\({''tag'': ''国足''}\\)\nif sample_with_time:\n    time_fields = [''create_time'', ''created_at'', ''timestamp'', ''publish_time'']\n    for field in time_fields:\n        if field in sample_with_time:\n            print\\(f''{field}: {sample_with_time[field]}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nfrom pymongo import MongoClient\n\nclient = MongoClient\\(''mongodb://localhost:27017/''\\)\ndb = client[''public_opinion_analysis_system'']\n\n# 查看国足数据的详细结构\nprint\\(''=== 国足数据详细结构 ===''\\)\nguozu_doc = db[''blog''].find_one\\({''tag'': ''国足''}\\)\nif guozu_doc:\n    print\\(f''tag_task_id: {guozu_doc.get\\(\"\"tag_task_id\"\"\\)}''\\)\n    data = guozu_doc.get\\(''data'', []\\)\n    print\\(f''data数组长度: {len\\(data\\)}''\\)\n    \n    if data:\n        print\\(''\\\\n第一条微博数据字段:''\\)\n        first_weibo = data[0]\n        for key, value in first_weibo.items\\(\\):\n            print\\(f''  {key}: {str\\(value\\)[:80]}''\\)\n        \n        # 检查时间字段\n        print\\(''\\\\n=== 时间字段分析 ===''\\)\n        time_fields = [''create_time'', ''created_at'', ''timestamp'', ''publish_time'']\n        for field in time_fields:\n            if field in first_weibo:\n                print\\(f''找到时间字段: {field} = {first_weibo[field]}''\\)\n        \n        # 统计时间分布\n        print\\(''\\\\n=== 微博时间分布 ===''\\)\n        from collections import Counter\n        times = []\n        for weibo in data[:20]:  # 只看前20条\n            ct = weibo.get\\(''create_time'', weibo.get\\(''created_at'', ''unknown''\\)\\)\n            times.append\\(ct\\)\n        for t in times:\n            print\\(f''  {t}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport datetime\n\n# 测试时间戳解析\ntimestamp = 1769415099048  # 这是数据库中的时间戳\n\n# 检查这个时间戳\nprint\\(f''原始时间戳: {timestamp}''\\)\nprint\\(f''时间戳长度: {len\\(str\\(timestamp\\)\\)}''\\)\n\n# 尝试解析\nif timestamp > 1e12:  # 毫秒时间戳\n    dt = datetime.datetime.fromtimestamp\\(timestamp / 1000\\)\n    print\\(f''解析为毫秒时间戳: {dt}''\\)\nelif timestamp > 1e9:  # 秒时间戳\n    dt = datetime.datetime.fromtimestamp\\(timestamp\\)\n    print\\(f''解析为秒时间戳: {dt}''\\)\n\n# 检查时间范围\nnow = datetime.datetime.now\\(\\)\none_day_ago = now - datetime.timedelta\\(days=1\\)\none_month_ago = now - datetime.timedelta\\(days=30\\)\nthree_month_ago = now - datetime.timedelta\\(days=90\\)\n\nprint\\(f''\\\\n当前时间: {now}''\\)\nprint\\(f''一天前: {one_day_ago}''\\)\nprint\\(f''一个月前: {one_month_ago}''\\)\nprint\\(f''三个月前: {three_month_ago}''\\)\n\n# 检查解析后的时间是否在范围内\nif timestamp > 1e12:\n    dt = datetime.datetime.fromtimestamp\\(timestamp / 1000\\)\n    print\\(f''\\\\n解析后的时间 {dt}:''\\)\n    print\\(f''  在一天内: {dt >= one_day_ago}''\\)\n    print\\(f''  在一个月内: {dt >= one_month_ago}''\\)\n    print\\(f''  在三个月内: {dt >= three_month_ago}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/back_end''\\)\n\nfrom celery_task.tag_task.tag_hot_task import calculate_local_hot_data\n\n# 测试本地热度计算\nresult = calculate_local_hot_data\\(''国足'', ''f0a31f388d113750fcaafae74b3ff218''\\)\nprint\\(''\\\\n=== 计算结果 ===''\\)\nprint\\(f''one_day: {result.get\\(\"\"one_day\"\"\\)}''\\)\nprint\\(f''one_month: {result.get\\(\"\"one_month\"\"\\)}''\\)\nprint\\(f''three_month: {result.get\\(\"\"three_month\"\"\\)}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nfrom pymongo import MongoClient\nimport datetime\nfrom collections import Counter\n\nclient = MongoClient\\(''mongodb://localhost:27017/''\\)\ndb = client[''public_opinion_analysis_system'']\n\n# 获取国足数据\nguozu_doc = db[''blog''].find_one\\({''tag'': ''国足''}\\)\ndata = guozu_doc.get\\(''data'', []\\)\n\nprint\\(f''总微博数: {len\\(data\\)}''\\)\n\n# 分析所有时间戳\ntimestamps = []\nfor weibo in data:\n    ct = weibo.get\\(''create_time''\\)\n    if ct:\n        timestamps.append\\(ct\\)\n\nprint\\(f''有时间戳的微博数: {len\\(timestamps\\)}''\\)\n\n# 检查时间戳范围\nif timestamps:\n    min_ts = min\\(timestamps\\)\n    max_ts = max\\(timestamps\\)\n    \n    # 转换为日期\n    min_dt = datetime.datetime.fromtimestamp\\(min_ts / 1000\\)\n    max_dt = datetime.datetime.fromtimestamp\\(max_ts / 1000\\)\n    \n    print\\(f''\\\\n时间戳范围:''\\)\n    print\\(f''  最早: {min_ts} -> {min_dt}''\\)\n    print\\(f''  最晚: {max_ts} -> {max_dt}''\\)\n    \n    # 按日期统计\n    date_counts = Counter\\(\\)\n    for ts in timestamps:\n        dt = datetime.datetime.fromtimestamp\\(ts / 1000\\)\n        date_str = dt.strftime\\(''%Y-%m-%d''\\)\n        date_counts[date_str] += 1\n    \n    print\\(f''\\\\n按日期统计:''\\)\n    for date, count in sorted\\(date_counts.items\\(\\)\\):\n        print\\(f''  {date}: {count}条''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nfrom pymongo import MongoClient\n\nclient = MongoClient\\(''mongodb://localhost:27017/''\\)\ndb = client[''public_opinion_analysis_system'']\n\n# 获取国足数据\nguozu_doc = db[''blog''].find_one\\({''tag'': ''国足''}\\)\ndata = guozu_doc.get\\(''data'', []\\)\n\nprint\\(''=== 检查 created_at 字段 ===''\\)\nfor i, weibo in enumerate\\(data[:10]\\):\n    created_at = weibo.get\\(''created_at'', ''N/A''\\)\n    create_time = weibo.get\\(''create_time'', ''N/A''\\)\n    print\\(f''{i+1}. created_at: \"\"{created_at}\"\" | create_time: {create_time}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/back_end''\\)\n\nfrom celery_task.tag_task.tag_hot_task import calculate_local_hot_data\n\n# 测试本地热度计算（中期方案）\nresult = calculate_local_hot_data\\(''国足'', ''f0a31f388d113750fcaafae74b3ff218''\\)\nprint\\(''\\\\n=== 计算结果 ===''\\)\nprint\\(f''one_day 天数: {len\\(result.get\\(\"\"one_day\"\", {}\\)\\)}''\\)\nprint\\(f''one_month 天数: {len\\(result.get\\(\"\"one_month\"\", {}\\)\\)}''\\)\nprint\\(f''three_month 天数: {len\\(result.get\\(\"\"three_month\"\", {}\\)\\)}''\\)\n\nprint\\(''\\\\n=== one_day 数据示例（前5天）===''\\)\none_day = result.get\\(''one_day'', {}\\)\nfor i, \\(date, heat\\) in enumerate\\(sorted\\(one_day.items\\(\\)\\)[:5]\\):\n    print\\(f''  {date}: {heat}''\\)\n\nprint\\(''\\\\n=== one_month 数据示例（前5天）===''\\)\none_month = result.get\\(''one_month'', {}\\)\nfor i, \\(date, heat\\) in enumerate\\(sorted\\(one_month.items\\(\\)\\)[:5]\\):\n    print\\(f''  {date}: {heat}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler''\\)\n\n# 测试爬取一个页面，看看原始HTML中的时间信息\nfrom web_curl import WebCurl\nfrom lxml import etree\n\n# 创建爬虫实例\ncrawler = WebCurl\\(\\)\n\n# 搜索国足相关微博\nurl = ''https://s.weibo.com/weibo?q=%E5%9B%BD%E8%B6%B3&page=1''\nprint\\(f''请求URL: {url}''\\)\n\ntry:\n    response = crawler.curl_get\\(url\\)\n    if response:\n        print\\(f''响应长度: {len\\(response\\)}''\\)\n        \n        # 解析HTML\n        selector = etree.HTML\\(response\\)\n        \n        # 查找时间相关的元素\n        print\\(''\\\\n=== 查找时间元素 ===''\\)\n        \n        # 方法1: 原有的xpath\n        from_elements = selector.xpath\\(''//p[@class=\"\"from\"\"]''\\)\n        print\\(f''找到 p.from 元素: {len\\(from_elements\\)}''\\)\n        \n        if from_elements:\n            for i, elem in enumerate\\(from_elements[:3]\\):\n                text = elem.xpath\\(''string\\(.\\)''\\)\n                links = elem.xpath\\(''.//a/text\\(\\)''\\)\n                print\\(f''  [{i}] 文本: {text.strip\\(\\)[:100]}''\\)\n                print\\(f''  [{i}] 链接文本: {links}''\\)\n        \n        # 方法2: 查找包含时间的span\n        time_spans = selector.xpath\\(''//span[contains\\(@class, \"\"time\"\"\\)]''\\)\n        print\\(f''\\\\n找到 span.time 元素: {len\\(time_spans\\)}''\\)\n        \n        # 方法3: 查找所有可能包含时间的元素\n        all_from_a = selector.xpath\\(''//p[@class=\"\"from\"\"]/a[1]''\\)\n        print\\(f''\\\\n找到 p.from/a[1] 元素: {len\\(all_from_a\\)}''\\)\n        if all_from_a:\n            for i, a in enumerate\\(all_from_a[:5]\\):\n                text = a.text or ''''\n                href = a.get\\(''href'', ''''\\)\n                print\\(f''  [{i}] text=\"\"{text.strip\\(\\)}\"\" href=\"\"{href[:50]}...\"\"''\\)\n        \n        # 打印一段原始HTML看看结构\n        print\\(''\\\\n=== 原始HTML片段 ===''\\)\n        cards = selector.xpath\\(''//div[@class=\"\"card-wrap\"\"]''\\)\n        if cards:\n            card_html = etree.tostring\\(cards[0], encoding=''unicode''\\)\n            # 只打印from部分\n            if ''<p class=\"\"from\"\"'' in card_html:\n                start = card_html.find\\(''<p class=\"\"from\"\"''\\)\n                end = card_html.find\\(''</p>'', start\\) + 4\n                print\\(card_html[start:end]\\)\n    else:\n        print\\(''请求失败''\\)\nexcept Exception as e:\n    print\\(f''错误: {e}''\\)\n    import traceback\n    traceback.print_exc\\(\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport requests\nfrom lxml import etree\nimport json\n\n# 读取cookie\nwith open\\(''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler/account/account.json'', ''r''\\) as f:\n    accounts = json.load\\(f\\)\n    cookie = accounts[0].get\\(''cookie'', ''''\\) if accounts else ''''\n\nprint\\(f''Cookie长度: {len\\(cookie\\)}''\\)\n\nheaders = {\n    ''User-Agent'': ''Mozilla/5.0 \\(Windows NT 10.0; Win64; x64\\) AppleWebKit/537.36 \\(KHTML, like Gecko\\) Chrome/120.0.0.0 Safari/537.36'',\n    ''Cookie'': cookie,\n    ''Accept'': ''text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'',\n    ''Accept-Language'': ''zh-CN,zh;q=0.9,en;q=0.8'',\n}\n\nurl = ''https://s.weibo.com/weibo?q=%E5%9B%BD%E8%B6%B3&page=1''\nprint\\(f''请求URL: {url}''\\)\n\ntry:\n    response = requests.get\\(url, headers=headers, timeout=30\\)\n    print\\(f''响应状态码: {response.status_code}''\\)\n    print\\(f''响应长度: {len\\(response.text\\)}''\\)\n    \n    # 解析HTML\n    selector = etree.HTML\\(response.text\\)\n    \n    # 查找时间相关的元素\n    print\\(''\\\\n=== 查找时间元素 ===''\\)\n    \n    # 方法1: 原有的xpath\n    from_elements = selector.xpath\\(''//p[@class=\"\"from\"\"]''\\)\n    print\\(f''找到 p.from 元素: {len\\(from_elements\\)}''\\)\n    \n    if from_elements:\n        for i, elem in enumerate\\(from_elements[:3]\\):\n            text = elem.xpath\\(''string\\(.\\)''\\).strip\\(\\)\n            links = elem.xpath\\(''.//a/text\\(\\)''\\)\n            print\\(f''  [{i}] 完整文本: {text[:100]}''\\)\n            print\\(f''  [{i}] 链接文本: {links}''\\)\n    \n    # 方法2: 查找 p.from 下的第一个 a 标签\n    all_from_a = selector.xpath\\(''//p[@class=\"\"from\"\"]/a[1]''\\)\n    print\\(f''\\\\n找到 p.from/a[1] 元素: {len\\(all_from_a\\)}''\\)\n    if all_from_a:\n        for i, a in enumerate\\(all_from_a[:5]\\):\n            text = \\(a.text or ''''\\).strip\\(\\)\n            href = a.get\\(''href'', ''''\\)\n            print\\(f''  [{i}] text=\"\"{text}\"\" href=\"\"{href[:60]}\"\"''\\)\n    \n    # 打印一段原始HTML看看结构\n    print\\(''\\\\n=== 原始HTML片段 \\(p.from\\) ===''\\)\n    if from_elements:\n        from_html = etree.tostring\\(from_elements[0], encoding=''unicode''\\)\n        print\\(from_html[:500]\\)\n        \nexcept Exception as e:\n    print\\(f''错误: {e}''\\)\n    import traceback\n    traceback.print_exc\\(\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport requests\nfrom lxml import etree\nimport json\nimport re\n\n# 读取cookie\nwith open\\(''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler/account/account.json'', ''r''\\) as f:\n    data = json.load\\(f\\)\n    cookie = data.get\\(''cookies'', ['''']\\)[0]\n\nheaders = {\n    ''User-Agent'': ''Mozilla/5.0 \\(Windows NT 10.0; Win64; x64\\) AppleWebKit/537.36 \\(KHTML, like Gecko\\) Chrome/120.0.0.0 Safari/537.36'',\n    ''Cookie'': cookie,\n}\n\nurl = ''https://s.weibo.com/weibo?q=%E5%9B%BD%E8%B6%B3&page=1''\n\nresponse = requests.get\\(url, headers=headers, timeout=30\\)\nselector = etree.HTML\\(response.text\\)\n\n# 查找card-wrap元素\ncards = selector.xpath\\(''//div[@class=\"\"card-wrap\"\"]''\\)\nprint\\(f''找到 card-wrap 元素: {len\\(cards\\)}''\\)\n\n# 查找card元素\ncards2 = selector.xpath\\(''//div[@class=\"\"card\"\"]''\\)\nprint\\(f''找到 card 元素: {len\\(cards2\\)}''\\)\n\n# 查找所有包含时间信息的可能元素\nprint\\(''\\\\n=== 搜索可能的时间元素 ===''\\)\n\n# 搜索包含时间格式的文本\ntime_patterns = [''分钟前'', ''小时前'', ''今天'', ''昨天'', ''月'', ''日'']\nfor pattern in time_patterns:\n    elements = selector.xpath\\(f''//*[contains\\(text\\(\\), \"\"{pattern}\"\"\\)]''\\)\n    if elements:\n        print\\(f''\\\\n包含 \"\"{pattern}\"\" 的元素: {len\\(elements\\)}''\\)\n        for i, elem in enumerate\\(elements[:2]\\):\n            tag = elem.tag\n            cls = elem.get\\(''class'', ''''\\)\n            text = \\(elem.text or ''''\\).strip\\(\\)[:50]\n            print\\(f''  [{i}] <{tag} class=\"\"{cls}\"\"> text=\"\"{text}\"\"''\\)\n\n# 打印第一个card的完整HTML结构\nif cards2:\n    print\\(''\\\\n=== 第一个card的HTML结构 ===''\\)\n    card_html = etree.tostring\\(cards2[0], encoding=''unicode'', pretty_print=True\\)\n    # 只打印前2000字符\n    print\\(card_html[:2000]\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport requests\nfrom lxml import etree\nimport json\n\n# 读取cookie\nwith open\\(''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler/account/account.json'', ''r''\\) as f:\n    data = json.load\\(f\\)\n    cookie = data.get\\(''cookies'', ['''']\\)[0]\n\nheaders = {\n    ''User-Agent'': ''Mozilla/5.0 \\(Windows NT 10.0; Win64; x64\\) AppleWebKit/537.36'',\n    ''Cookie'': cookie,\n}\n\nurl = ''https://s.weibo.com/weibo?q=%E5%9B%BD%E8%B6%B3&page=1''\nresponse = requests.get\\(url, headers=headers, timeout=30\\)\nselector = etree.HTML\\(response.text\\)\n\n# 找到包含时间的a标签的父元素\nprint\\(''=== 分析时间元素的父级结构 ===''\\)\n\n# 找到包含 ''分钟前'' 的a标签\ntime_links = selector.xpath\\(''//a[contains\\(text\\(\\), \"\"分钟前\"\"\\) or contains\\(text\\(\\), \"\"小时前\"\"\\) or contains\\(text\\(\\), \"\"月\"\"\\) and contains\\(text\\(\\), \"\"日\"\"\\)]''\\)\nprint\\(f''找到时间链接: {len\\(time_links\\)}''\\)\n\nif time_links:\n    for i, a in enumerate\\(time_links[:3]\\):\n        text = \\(a.text or ''''\\).strip\\(\\)\n        href = a.get\\(''href'', ''''\\)\n        parent = a.getparent\\(\\)\n        parent_tag = parent.tag if parent is not None else ''None''\n        parent_class = parent.get\\(''class'', ''''\\) if parent is not None else ''''\n        \n        # 获取祖父元素\n        grandparent = parent.getparent\\(\\) if parent is not None else None\n        gp_tag = grandparent.tag if grandparent is not None else ''None''\n        gp_class = grandparent.get\\(''class'', ''''\\) if grandparent is not None else ''''\n        \n        print\\(f''[{i}] text=\"\"{text}\"\"''\\)\n        print\\(f''    href=\"\"{href[:80]}\"\"''\\)\n        print\\(f''    parent: <{parent_tag} class=\"\"{parent_class}\"\">''\\)\n        print\\(f''    grandparent: <{gp_tag} class=\"\"{gp_class}\"\">''\\)\n        print\\(\\)\n\n# 尝试新的xpath\nprint\\(''=== 测试新的XPath ===''\\)\n\n# 方法1: 直接找card-feed下的时间链接\nxpath1 = ''//div[@class=\"\"card-feed\"\"]//a[contains\\(@href, \"\"weibo.com\"\"\\) and \\(contains\\(text\\(\\), \"\"分钟前\"\"\\) or contains\\(text\\(\\), \"\"小时前\"\"\\) or contains\\(text\\(\\), \"\"月\"\"\\)\\)]''\nresults1 = selector.xpath\\(xpath1\\)\nprint\\(f''XPath1 结果: {len\\(results1\\)}''\\)\nfor r in results1[:3]:\n    print\\(f''  {\\(r.text or \"\"\"\"\\).strip\\(\\)[:50]}''\\)\n\n# 方法2: 找card下的时间链接（排除用户链接）\nxpath2 = ''//div[@class=\"\"card\"\"]//a[contains\\(text\\(\\), \"\"分钟前\"\"\\) or contains\\(text\\(\\), \"\"小时前\"\"\\) or \\(contains\\(text\\(\\), \"\"月\"\"\\) and contains\\(text\\(\\), \"\"日\"\"\\)\\)]''\nresults2 = selector.xpath\\(xpath2\\)\nprint\\(f''\\\\nXPath2 结果: {len\\(results2\\)}''\\)\nfor r in results2[:5]:\n    print\\(f''  {\\(r.text or \"\"\"\"\\).strip\\(\\)[:50]}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler''\\)\n\nfrom utils import standardize_date\n\n# 测试各种时间格式\ntest_cases = [\n    ''刚刚'',\n    ''5分钟'',  # 注意：爬虫会去掉''前''字\n    ''2小时'',\n    ''今天10:30'',\n    ''昨天18:45'',\n    ''01月25日06:49'',\n    ''01月25日 06:49'',\n    ''1月5日12:00'',\n    ''12月31日23:59'',\n    ''2026-01-25 10:30'',\n]\n\nprint\\(''=== 测试 standardize_date 函数 ===''\\)\nfor tc in test_cases:\n    result = standardize_date\\(tc\\)\n    print\\(f''  \"\"{tc}\"\" -> \"\"{result}\"\"''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler''\\)\n\nfrom utils import standardize_date\n\n# 测试各种时间格式\ntest_cases = [\n    ''刚刚'',\n    ''5分钟'',\n    ''2小时'',\n    ''今天10:30'',\n    ''昨天18:45'',\n    ''01月25日06:49'',\n    ''01月25日 06:49'',\n    ''1月5日12:00'',\n    ''12月31日23:59'',\n    ''2026-01-25 10:30'',\n    ''2026-01-25'',\n]\n\nprint\\(''=== 测试 standardize_date 函数 ===''\\)\nfor tc in test_cases:\n    result = standardize_date\\(tc\\)\n    print\\(f''  \"\"{tc}\"\" -> \"\"{result}\"\"''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler''\\)\n\nimport requests\nfrom lxml import etree\nimport json\nfrom selector_parser.search_weibo_parser import SearchWeiboParser\n\n# 读取cookie\nwith open\\(''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler/account/account.json'', ''r''\\) as f:\n    data = json.load\\(f\\)\n    cookie = data.get\\(''cookies'', ['''']\\)[0]\n\nheaders = {\n    ''User-Agent'': ''Mozilla/5.0 \\(Windows NT 10.0; Win64; x64\\) AppleWebKit/537.36'',\n    ''Cookie'': cookie,\n}\n\nurl = ''https://s.weibo.com/weibo?q=%E5%9B%BD%E8%B6%B3&page=1''\nprint\\(f''请求URL: {url}''\\)\n\nresponse = requests.get\\(url, headers=headers, timeout=30\\)\nprint\\(f''响应状态码: {response.status_code}''\\)\n\n# 使用解析器\nclass MockResponse:\n    def __init__\\(self, body\\):\n        self.body = body.encode\\(''utf-8''\\)\n\nmock_resp = MockResponse\\(response.text\\)\nparser = SearchWeiboParser\\(mock_resp\\)\n\ntry:\n    weibo_list = parser.parse_page\\(\\)\n    if weibo_list:\n        print\\(f''\\\\n成功解析 {len\\(weibo_list\\)} 条微博''\\)\n        print\\(''\\\\n=== 前5条微博的时间信息 ===''\\)\n        for i, weibo in enumerate\\(weibo_list[:5]\\):\n            created_at = weibo.get\\(''created_at'', ''N/A''\\)\n            screen_name = weibo.get\\(''screen_name'', ''N/A''\\)\n            text = weibo.get\\(''text'', ''''\\)[:30]\n            print\\(f''[{i+1}] {screen_name}: created_at=\"\"{created_at}\"\"''\\)\n            print\\(f''     text: {text}...''\\)\n    else:\n        print\\(''解析结果为空''\\)\nexcept Exception as e:\n    print\\(f''解析错误: {e}''\\)\n    import traceback\n    traceback.print_exc\\(\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nfrom pymongo import MongoClient\n\nclient = MongoClient\\(''mongodb://localhost:27017/''\\)\ndb = client[''public_opinion_analysis_system'']\n\n# 查看tag_hot中的source字段\nprint\\(''=== tag_hot 数据来源 ===''\\)\nfor doc in db[''tag_hot''].find\\(\\):\n    tag = doc.get\\(''tag'', ''N/A''\\)\n    source = doc.get\\(''source'', ''未标记''\\)\n    one_day_len = len\\(doc.get\\(''one_day'', {}\\)\\) if isinstance\\(doc.get\\(''one_day''\\), dict\\) else len\\(doc.get\\(''one_day'', []\\)\\)\n    three_month_len = len\\(doc.get\\(''three_month'', {}\\)\\) if isinstance\\(doc.get\\(''three_month''\\), dict\\) else len\\(doc.get\\(''three_month'', []\\)\\)\n    print\\(f''话题: {tag}''\\)\n    print\\(f''  来源: {source}''\\)\n    print\\(f''  one_day数据点: {one_day_len}, three_month数据点: {three_month_len}''\\)\n    print\\(\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler''\\)\n\nimport requests\nfrom lxml import etree\nimport json\nfrom selector_parser.search_weibo_parser import SearchWeiboParser\n\n# 读取cookie\nwith open\\(''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler/account/account.json'', ''r''\\) as f:\n    data = json.load\\(f\\)\n    cookie = data.get\\(''cookies'', ['''']\\)[0]\n\nheaders = {\n    ''User-Agent'': ''Mozilla/5.0 \\(Windows NT 10.0; Win64; x64\\) AppleWebKit/537.36'',\n    ''Cookie'': cookie,\n}\n\n# 爬取\"放假\"话题\nurl = ''https://s.weibo.com/weibo?q=%E6%94%BE%E5%81%87&page=1''\nprint\\(f''爬取话题: 放假''\\)\nprint\\(f''URL: {url}''\\)\n\nresponse = requests.get\\(url, headers=headers, timeout=30\\)\nprint\\(f''响应状态码: {response.status_code}''\\)\n\n# 使用解析器\nclass MockResponse:\n    def __init__\\(self, body\\):\n        self.body = body.encode\\(''utf-8''\\)\n\nmock_resp = MockResponse\\(response.text\\)\nparser = SearchWeiboParser\\(mock_resp\\)\n\nweibo_list = parser.parse_page\\(\\)\nif weibo_list:\n    print\\(f''\\\\n成功解析 {len\\(weibo_list\\)} 条微博''\\)\n    \n    # 统计时间分布\n    from collections import Counter\n    dates = []\n    print\\(''\\\\n=== 微博时间数据 ===''\\)\n    for i, weibo in enumerate\\(weibo_list[:10]\\):\n        created_at = weibo.get\\(''created_at'', ''''\\)\n        screen_name = weibo.get\\(''screen_name'', ''N/A''\\)\n        print\\(f''[{i+1}] {screen_name}: {created_at}''\\)\n        if created_at:\n            date_part = created_at.split\\('' ''\\)[0]\n            dates.append\\(date_part\\)\n    \n    print\\(f''\\\\n=== 时间分布统计 ===''\\)\n    date_counter = Counter\\(dates\\)\n    for date, count in sorted\\(date_counter.items\\(\\)\\):\n        print\\(f''  {date}: {count}条''\\)\n    \n    print\\(f''\\\\n共有 {len\\(set\\(dates\\)\\)} 个不同的日期''\\)\nelse:\n    print\\(''解析结果为空''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler''\\)\n\nfrom utils import standardize_date\n\n# 测试有问题的格式\ntest_cases = [\n    ''2025年03月31日09:49'',  # 可能是这种格式\n    ''03月31日09:49'',\n    ''2025-03-31 09:49'',\n]\n\nprint\\(''=== 测试时间格式 ===''\\)\nfor tc in test_cases:\n    result = standardize_date\\(tc\\)\n    print\\(f''\"\"{tc}\"\" -> \"\"{result}\"\"''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler''\\)\n\nfrom utils import standardize_date\n\n# 测试所有格式\ntest_cases = [\n    ''刚刚'',\n    ''5分钟'',\n    ''2小时'',\n    ''今天10:30'',\n    ''昨天18:45'',\n    ''01月25日06:49'',\n    ''2025年03月31日09:49'',\n    ''2025-03-31 09:49'',\n]\n\nprint\\(''=== 测试 standardize_date ===''\\)\nfor tc in test_cases:\n    result = standardize_date\\(tc\\)\n    print\\(f''\"\"{tc}\"\" -> \"\"{result}\"\"''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler''\\)\n\nimport requests\nimport json\nfrom selector_parser.search_weibo_parser import SearchWeiboParser\nfrom collections import Counter\n\n# 读取cookie\nwith open\\(''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/weibo_crawler/account/account.json'', ''r''\\) as f:\n    data = json.load\\(f\\)\n    cookie = data.get\\(''cookies'', ['''']\\)[0]\n\nheaders = {\n    ''User-Agent'': ''Mozilla/5.0 \\(Windows NT 10.0; Win64; x64\\) AppleWebKit/537.36'',\n    ''Cookie'': cookie,\n}\n\nclass MockResponse:\n    def __init__\\(self, body\\):\n        self.body = body.encode\\(''utf-8''\\)\n\n# 爬取多页数据\nall_weibos = []\nfor page in range\\(1, 4\\):  # 爬取3页\n    url = f''https://s.weibo.com/weibo?q=%E6%94%BE%E5%81%87&page={page}''\n    response = requests.get\\(url, headers=headers, timeout=30\\)\n    \n    mock_resp = MockResponse\\(response.text\\)\n    parser = SearchWeiboParser\\(mock_resp\\)\n    weibo_list = parser.parse_page\\(\\)\n    \n    if weibo_list:\n        all_weibos.extend\\(weibo_list\\)\n        print\\(f''第{page}页: {len\\(weibo_list\\)}条''\\)\n    else:\n        print\\(f''第{page}页: 无数据''\\)\n\nprint\\(f''\\\\n总计: {len\\(all_weibos\\)}条微博''\\)\n\n# 统计时间分布\ndates = []\nvalid_count = 0\nfor weibo in all_weibos:\n    created_at = weibo.get\\(''created_at'', ''''\\)\n    if created_at:\n        valid_count += 1\n        date_part = created_at.split\\('' ''\\)[0]\n        dates.append\\(date_part\\)\n\nprint\\(f''有效时间数据: {valid_count}/{len\\(all_weibos\\)}''\\)\n\nprint\\(f''\\\\n=== 时间分布 ===''\\)\ndate_counter = Counter\\(dates\\)\nfor date, count in sorted\\(date_counter.items\\(\\)\\):\n    print\\(f''  {date}: {count}条''\\)\n\nprint\\(f''\\\\n共有 {len\\(set\\(dates\\)\\)} 个不同的日期''\\)\nprint\\(f''时间数据有效率: {valid_count/len\\(all_weibos\\)*100:.1f}%''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nfrom pymongo import MongoClient\n\nclient = MongoClient\\(''mongodb://localhost:27017/''\\)\ndb = client[''public_opinion_analysis_system'']\n\n# 查看最新的tag_hot数据\nprint\\(''=== tag_hot 数据 ===''\\)\nfor doc in db[''tag_hot''].find\\(\\).sort\\(''_id'', -1\\).limit\\(5\\):\n    tag = doc.get\\(''tag'', ''N/A''\\)\n    source = doc.get\\(''source'', ''未标记''\\)\n    task_id = doc.get\\(''tag_task_id'', ''N/A''\\)\n    print\\(f''话题: {tag}, 来源: {source}, task_id: {task_id[:20]}...''\\)\n\n# 查看对应的blog数据中的时间分布\nprint\\(''\\\\n=== 检查blog中的时间数据 ===''\\)\nfor doc in db[''blog''].find\\(\\).limit\\(3\\):\n    tag = doc.get\\(''tag'', ''N/A''\\)\n    data = doc.get\\(''data'', []\\)\n    if data:\n        # 检查时间字段\n        dates = set\\(\\)\n        for weibo in data[:50]:\n            created_at = weibo.get\\(''created_at'', ''''\\)\n            if created_at and isinstance\\(created_at, str\\) and len\\(created_at\\) > 5:\n                date_part = created_at.split\\('' ''\\)[0] if '' '' in created_at else created_at[:10]\n                dates.add\\(date_part\\)\n        print\\(f''话题: {tag}, 微博数: {len\\(data\\)}, 不同日期数: {len\\(dates\\)}''\\)\n        if dates:\n            print\\(f''  日期示例: {list\\(dates\\)[:5]}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/back_end''\\)\n\nfrom celery_task.tag_task.tag_hot_task import parse_weibo_time\n\n# 测试各种格式\ntest_cases = [\n    ''01月10日'',\n    ''2025年12月04日'',\n    ''今天14:27'',\n    ''2026-01-25 06:49'',\n    1769415099048,  # 毫秒时间戳\n]\n\nprint\\(''=== 测试 parse_weibo_time ===''\\)\nfor tc in test_cases:\n    result = parse_weibo_time\\(tc\\)\n    print\\(f''{repr\\(tc\\)} -> {result}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport inspect\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/back_end''\\)\n\nfrom celery_task.tag_task.tag_hot_task import parse_weibo_time\nprint\\(inspect.getsource\\(parse_weibo_time\\)\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/back_end''\\)\n\n# 重新加载模块\nimport importlib\nfrom celery_task.tag_task import tag_hot_task\nimportlib.reload\\(tag_hot_task\\)\nfrom celery_task.tag_task.tag_hot_task import parse_weibo_time\n\n# 测试各种格式\ntest_cases = [\n    ''01月10日'',\n    ''01月10日 14:30'',\n    ''2025年12月04日'',\n    ''2025年12月04日 09:30'',\n    ''今天14:27'',\n    ''昨天18:45'',\n    ''2026-01-25 06:49'',\n    ''2026-01-25'',\n    1769415099048,\n]\n\nprint\\(''=== 测试 parse_weibo_time ===''\\)\nfor tc in test_cases:\n    result = parse_weibo_time\\(tc\\)\n    print\\(f''{repr\\(tc\\):30} -> {result}''\\)\n\")",
      "Bash(conda run -n graduation python -c \"\nimport sys\nsys.path.insert\\(0, ''/home/song/code/graduation/Topic_and_user_profile_analysis_system/code/back_end''\\)\n\n# 重新加载模块\nimport importlib\nfrom celery_task.tag_task import tag_hot_task\nimportlib.reload\\(tag_hot_task\\)\nfrom celery_task.tag_task.tag_hot_task import calculate_local_hot_data\n\n# 测试一个有真实时间数据的话题\nresult = calculate_local_hot_data\\(''临榆炸鸡腿'', ''test''\\)\nprint\\(''\\\\n=== 计算结果 ===''\\)\nprint\\(f''one_day 天数: {len\\(result.get\\(\"\"one_day\"\", {}\\)\\)}''\\)\nprint\\(f''one_month 天数: {len\\(result.get\\(\"\"one_month\"\", {}\\)\\)}''\\)\nprint\\(f''three_month 天数: {len\\(result.get\\(\"\"three_month\"\", {}\\)\\)}''\\)\n\nif result.get\\(''three_month''\\):\n    print\\(''\\\\n=== three_month 数据示例 ===''\\)\n    for i, \\(date, heat\\) in enumerate\\(sorted\\(result[''three_month''].items\\(\\)\\)[:10]\\):\n        print\\(f''  {date}: {heat}''\\)\n\")"
    ]
  }
}
